{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyFRCXiLo1IPXm6LqbkKF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bais2004/1st_Demo/blob/main/Detecting_Fake_job_post_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14-10-2025"
      ],
      "metadata": {
        "id": "yqTKM7TQG4o2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdHpIVQA374E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741857f1-6a91-40f3-a515-6500f4ff678d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Data', 'Science', 'is', 'fun', '!', 'My', 'name', 'is', 'Mritunjay', '.', 'I', 'am', 'from', 'Bhopal', '.', 'I', 'am', 'prusing', 'my', 'bachelors', 'degree', 'from', 'Technocrats', 'Institute', 'of', 'Technology', 'in', 'Artificial', 'Intelligence', 'And', 'Machine', 'Learning', '.']\n",
            "33\n"
          ]
        }
      ],
      "source": [
        "# TOKENIZATION\n",
        "import nltk # natural lang. toolkit\n",
        "nltk.download('punkt') # Download tokenizer data\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"Data Science is fun!\"\n",
        "text1= \"My name is Mritunjay. I am from Bhopal. I am prusing my bachelors degree from Technocrats Institute of Technology in Artificial Intelligence And Machine Learning.\"\n",
        "\n",
        "# Tokenize text\n",
        "tokens = word_tokenize(text)\n",
        "tokens1 = word_tokenize(text1)\n",
        "\n",
        "print(\"Tokens:\", tokens+tokens1)\n",
        "print(len(tokens + tokens1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15-10-2025\n"
      ],
      "metadata": {
        "id": "mR3LG6V0HDl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Create stemmer\n",
        "stemmer = PorterStemmer()\n",
        "# Sample text\n",
        "text = \"lle was studies and beautifully explained it easily.\"\n",
        "text1 = \"Artificial intelligence is studies  transforming the world  and display each word as a¬†separate¬†token\"\n",
        "tokens = word_tokenize(text1)\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Original Words:\", tokens)\n",
        "print(\"After Stemming:\", stemmed_words)"
      ],
      "metadata": {
        "id": "P6bLkRT9A00U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c577afcd-eaae-4268-fc58-ef80b9cf2a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['Artificial', 'intelligence', 'is', 'studies', 'transforming', 'the', 'world', 'and', 'display', 'each', 'word', 'as', 'a', 'separate', 'token']\n",
            "After Stemming: ['artifici', 'intellig', 'is', 'studi', 'transform', 'the', 'world', 'and', 'display', 'each', 'word', 'as', 'a', 'separ', 'token']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LEMMATIZATION\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"omw-1.4\")\n",
        "#Create lemmatizer\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "#Sample text\n",
        "text = \"The studies were running better than expected boys gives given played.\"\n",
        "tokens =word_tokenize(text)\n",
        "#Apply lemmatization\n",
        "lem_words =[lemmatizer.lemmatize(word) for word in tokens]\n",
        "#lem_words1 =[lemmatizer.lemmatize(word) for word in lem_words]\n",
        "print(\"orginal words :\",tokens)\n",
        "print(\"After lemmatization:\", (lem_words))\n",
        "\n"
      ],
      "metadata": {
        "id": "mYHEbRUxFWii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47b40af-524c-4188-fec3-057b3c4f1649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orginal words : ['The', 'studies', 'were', 'running', 'better', 'than', 'expected', 'boys', 'gives', 'given', 'played', '.']\n",
            "After lemmatization: ['The', 'study', 'were', 'running', 'better', 'than', 'expected', 'boy', 'give', 'given', 'played', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"omw-1.4\")\n",
        "#Create lemmatizer\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "#create Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "#Sample text\n",
        "text = input(\"Enter your Sentence ->\")\n",
        "tokens =word_tokenize(text)\n",
        "#Apply lemmatization\n",
        "lem_words =[lemmatizer.lemmatize(word) for word in tokens]\n",
        "#Apply Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"orginal words :\",tokens)\n",
        "print(\"After Stemming:\", stemmed_words)\n",
        "print(\"After lemmatization:\", lem_words)"
      ],
      "metadata": {
        "id": "DZdcNpi6ISTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11591d5c-5ae5-4be9-cb1d-e0d2a6eee9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Sentence ->sds asdg\n",
            "orginal words : ['sds', 'asdg']\n",
            "After Stemming: ['sd', 'asdg']\n",
            "After lemmatization: ['sd', 'asdg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "#Sample text\n",
        "text = \"The students are studying NLP in the lab.\"\n",
        "text1 = \"Artificial Intelligence is transforming the world rapidly.\"\n",
        "#Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "tokens1 = word_tokenize(text1)\n",
        "#Apply POS tagging\n",
        "pos_tags=nltk.pos_tag(tokens)\n",
        "pos_tags1 = nltk.pos_tag(tokens1)\n",
        "print(pos_tags)\n",
        "print(pos_tags1)"
      ],
      "metadata": {
        "id": "yW1vIaiNQn-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b3f090-8e65-437c-81b3-d5ac8ea90efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('students', 'NNS'), ('are', 'VBP'), ('studying', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('lab', 'NN'), ('.', '.')]\n",
            "[('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('transforming', 'VBG'), ('the', 'DT'), ('world', 'NN'), ('rapidly', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Day2 : Understanding and Loading the Dataset\n",
        "import pandas as pd\n",
        "# Load dataset (after downloading from Kaggle)\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "#df = pd.read_csv('/job_descriptions.csv')\n",
        "# Display total number of records\n",
        "print(\"Total number of records:\", len(df))\n",
        "# Display basic info\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum())\n",
        "# Check distribution of target variable (assuming 'Work Type' or similar can indicate fake jobs for this dataset)\n",
        "# Since there is no 'fraudulent' column, I'll skip this part for now.\n",
        "# If you have a column that indicates real vs. fake jobs, please let me know its name.\n",
        "# print(\"\\nTarget (fraudulent) Distribution:\")\n",
        "# print(df['fraudulent'].value_counts()) # This line caused an error\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nDataset Summary:\")\n",
        "print(df.describe(include='all'))\n",
        "\n",
        "# Display 3 examples of job descriptions (adjust column name if needed)\n",
        "print(\"\\nExamples of Job Descriptions:\")\n",
        "# Assuming 'Job Description' column exists based on df.columns output in cell YLWMaxC9DzGD\n",
        "if 'Job Description' in df.columns:\n",
        "    # Filter for potential \"fake\" job descriptions if a relevant column exists.\n",
        "    # Since there is no direct 'fraudulent' column, we'll just show general examples.\n",
        "    print(df['Job Description'].head(3).to_string())\n",
        "else:\n",
        "    print(\" 'Job Description' column not found.\")"
      ],
      "metadata": {
        "id": "q1-UInW766v4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaff0747-d720-41e8-b8de-0690d65cb200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of records: 17880\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17880 entries, 0 to 17879\n",
            "Data columns (total 18 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   job_id               17880 non-null  int64 \n",
            " 1   title                17880 non-null  object\n",
            " 2   location             17534 non-null  object\n",
            " 3   department           6333 non-null   object\n",
            " 4   salary_range         2868 non-null   object\n",
            " 5   company_profile      14572 non-null  object\n",
            " 6   description          17879 non-null  object\n",
            " 7   requirements         15184 non-null  object\n",
            " 8   benefits             10668 non-null  object\n",
            " 9   telecommuting        17880 non-null  int64 \n",
            " 10  has_company_logo     17880 non-null  int64 \n",
            " 11  has_questions        17880 non-null  int64 \n",
            " 12  employment_type      14409 non-null  object\n",
            " 13  required_experience  10830 non-null  object\n",
            " 14  required_education   9775 non-null   object\n",
            " 15  industry             12977 non-null  object\n",
            " 16  function             11425 non-null  object\n",
            " 17  fraudulent           17880 non-null  int64 \n",
            "dtypes: int64(5), object(13)\n",
            "memory usage: 2.5+ MB\n",
            "None\n",
            "\n",
            "Missing Values per Column:\n",
            "job_id                     0\n",
            "title                      0\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2696\n",
            "benefits                7212\n",
            "telecommuting              0\n",
            "has_company_logo           0\n",
            "has_questions              0\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "fraudulent                 0\n",
            "dtype: int64\n",
            "\n",
            "Dataset Summary:\n",
            "              job_id                    title         location department  \\\n",
            "count   17880.000000                    17880            17534       6333   \n",
            "unique           NaN                    11231             3105       1337   \n",
            "top              NaN  English Teacher Abroad   GB, LND, London      Sales   \n",
            "freq             NaN                      311              718        551   \n",
            "mean     8940.500000                      NaN              NaN        NaN   \n",
            "std      5161.655742                      NaN              NaN        NaN   \n",
            "min         1.000000                      NaN              NaN        NaN   \n",
            "25%      4470.750000                      NaN              NaN        NaN   \n",
            "50%      8940.500000                      NaN              NaN        NaN   \n",
            "75%     13410.250000                      NaN              NaN        NaN   \n",
            "max     17880.000000                      NaN              NaN        NaN   \n",
            "\n",
            "       salary_range                                    company_profile  \\\n",
            "count          2868                                              14572   \n",
            "unique          874                                               1709   \n",
            "top             0-0  We help teachers get safe &amp; secure jobs ab...   \n",
            "freq            142                                                726   \n",
            "mean            NaN                                                NaN   \n",
            "std             NaN                                                NaN   \n",
            "min             NaN                                                NaN   \n",
            "25%             NaN                                                NaN   \n",
            "50%             NaN                                                NaN   \n",
            "75%             NaN                                                NaN   \n",
            "max             NaN                                                NaN   \n",
            "\n",
            "                                              description  \\\n",
            "count                                               17879   \n",
            "unique                                              14801   \n",
            "top     Play with kids, get paid for it¬†Love travel? J...   \n",
            "freq                                                  379   \n",
            "mean                                                  NaN   \n",
            "std                                                   NaN   \n",
            "min                                                   NaN   \n",
            "25%                                                   NaN   \n",
            "50%                                                   NaN   \n",
            "75%                                                   NaN   \n",
            "max                                                   NaN   \n",
            "\n",
            "                                             requirements  \\\n",
            "count                                               15184   \n",
            "unique                                              11965   \n",
            "top     University degree required.¬†TEFL / TESOL / CEL...   \n",
            "freq                                                  410   \n",
            "mean                                                  NaN   \n",
            "std                                                   NaN   \n",
            "min                                                   NaN   \n",
            "25%                                                   NaN   \n",
            "50%                                                   NaN   \n",
            "75%                                                   NaN   \n",
            "max                                                   NaN   \n",
            "\n",
            "                   benefits  telecommuting  has_company_logo  has_questions  \\\n",
            "count                 10668   17880.000000      17880.000000   17880.000000   \n",
            "unique                 6203            NaN               NaN            NaN   \n",
            "top     See job description            NaN               NaN            NaN   \n",
            "freq                    726            NaN               NaN            NaN   \n",
            "mean                    NaN       0.042897          0.795302       0.491723   \n",
            "std                     NaN       0.202631          0.403492       0.499945   \n",
            "min                     NaN       0.000000          0.000000       0.000000   \n",
            "25%                     NaN       0.000000          1.000000       0.000000   \n",
            "50%                     NaN       0.000000          1.000000       0.000000   \n",
            "75%                     NaN       0.000000          1.000000       1.000000   \n",
            "max                     NaN       1.000000          1.000000       1.000000   \n",
            "\n",
            "       employment_type required_experience required_education  \\\n",
            "count            14409               10830               9775   \n",
            "unique               5                   7                 13   \n",
            "top          Full-time    Mid-Senior level  Bachelor's Degree   \n",
            "freq             11620                3809               5145   \n",
            "mean               NaN                 NaN                NaN   \n",
            "std                NaN                 NaN                NaN   \n",
            "min                NaN                 NaN                NaN   \n",
            "25%                NaN                 NaN                NaN   \n",
            "50%                NaN                 NaN                NaN   \n",
            "75%                NaN                 NaN                NaN   \n",
            "max                NaN                 NaN                NaN   \n",
            "\n",
            "                                   industry                function  \\\n",
            "count                                 12977                   11425   \n",
            "unique                                  131                      37   \n",
            "top     Information Technology and Services  Information Technology   \n",
            "freq                                   1734                    1749   \n",
            "mean                                    NaN                     NaN   \n",
            "std                                     NaN                     NaN   \n",
            "min                                     NaN                     NaN   \n",
            "25%                                     NaN                     NaN   \n",
            "50%                                     NaN                     NaN   \n",
            "75%                                     NaN                     NaN   \n",
            "max                                     NaN                     NaN   \n",
            "\n",
            "          fraudulent  \n",
            "count   17880.000000  \n",
            "unique           NaN  \n",
            "top              NaN  \n",
            "freq             NaN  \n",
            "mean        0.048434  \n",
            "std         0.214688  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max         1.000000  \n",
            "\n",
            "Examples of Job Descriptions:\n",
            " 'Job Description' column not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "YLWMaxC9DzGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3463bd-8361-45ea-b4a6-28de01974aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['job_id', 'title', 'location', 'department', 'salary_range',\n",
              "       'company_profile', 'description', 'requirements', 'benefits',\n",
              "       'telecommuting', 'has_company_logo', 'has_questions', 'employment_type',\n",
              "       'required_experience', 'required_education', 'industry', 'function',\n",
              "       'fraudulent'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Day 3: Text Cleaning and Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Download resources (run once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Load dataset (same as Day 2)\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "# Define text cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "    # 2. Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    # 3. Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    # 4. Remove punctuation and numbers\n",
        "    text = re.sub(r'[%s\\d]' % re.escape(string.punctuation), ' ', text)\n",
        "    # 5. Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # 6. Remove stopwords and lemmatize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "# Apply cleaning to key text columns\n",
        "df['clean_description'] = df['description'].apply(clean_text)\n",
        "# Show before and after\n",
        "print(\"Original Text:\\n\", df['description'].iloc[1][:300])\n",
        "print(\"\\nCleaned Text:\\n\", df['clean_description'].iloc[1][:300])\n",
        "# Check for any remaining issues\n",
        "print(\"\\nExample of Cleaned Data:\")\n",
        "print(df[['description', 'clean_description']].head(3))"
      ],
      "metadata": {
        "id": "CKLidUuFgPSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdca3b5-0f1e-489e-8b14-4e96462bdd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Organised - Focused - Vibrant - Awesome!Do you have a passion for customer service? Slick typing skills? Maybe Account Management? ...And think administration is cooler than a polar bear on a jetski? Then we need to hear you!¬†We are the Cloud Video Production Service and opperating on a glodal level\n",
            "\n",
            "Cleaned Text:\n",
            " organised focused vibrant awesome passion customer service slick typing skill maybe account management think administration cooler polar bear jetski need hear cloud video production service opperating glodal level yeah pretty cool serious delivering world class product excellent customer service rap\n",
            "\n",
            "Example of Cleaned Data:\n",
            "                                         description  \\\n",
            "0  Food52, a fast-growing, James Beard Award-winn...   \n",
            "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
            "2  Our client, located in Houston, is actively se...   \n",
            "\n",
            "                                   clean_description  \n",
            "0  food fast growing james beard award winning on...  \n",
            "1  organised focused vibrant awesome passion cust...  \n",
            "2  client located houston actively seeking experi...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DAY 4\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Load dataset\n",
        "# -----------------------------\n",
        "# Make sure the CSV file is in your working directory\n",
        "df = pd.read_csv('fake_job_postings.csv')\n",
        "# Check what columns exist\n",
        "print(\"Available columns:\", df.columns.tolist())\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Create clean_description column\n",
        "# -----------------------------\n",
        "# Use 'description' column (update name if different in your dataset)\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # keep only letters and spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra spaces\n",
        "    return text\n",
        "# Apply text cleaning\n",
        "df['clean_description'] = df['description'].apply(clean_text)\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Prepare text data\n",
        "# -----------------------------\n",
        "texts = df['clean_description'].fillna('').tolist()\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Bag-of-Words Representation\n",
        "# -----------------------------\n",
        "bow_vectorizer = CountVectorizer(max_features=2000)\n",
        "X_bow = bow_vectorizer.fit_transform(texts)\n",
        "print(\"‚úÖ BoW shape:\", X_bow.shape)\n",
        "print(\"üî§ Sample feature names (BoW):\", bow_vectorizer.get_feature_names_out()[:10])\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ TF-IDF Representation\n",
        "# -----------------------------\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
        "print(\"\\n‚úÖ TF-IDF shape:\", X_tfidf.shape)\n",
        "print(\"üî§ Sample feature names (TF-IDF):\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Compare first example\n",
        "# -----------------------------\n",
        "print(\"\\nüìò Example BoW vector (first row):\")\n",
        "print(X_bow[0].toarray())\n",
        "print(\"\\nüìó Example TF-IDF vector (first row):\")\n",
        "print(X_tfidf[0].toarray())"
      ],
      "metadata": {
        "id": "KqVCYvCBQHSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, html\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "df = pd.read_csv(\"fake_job_postings.csv\")\n",
        "\n",
        "print(\"Total Records:\", len(df))\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "\n",
        "print(\"\\nTarget Distribution (fraudulent column):\")\n",
        "print(df['fraudulent'].value_counts(normalize=True))\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean raw job posting text:\n",
        "    - Unescape HTML\n",
        "    - Lowercase\n",
        "    - Remove URLs, emails, numbers, punctuation\n",
        "    - Tokenize\n",
        "    - Remove stopwords\n",
        "    - Lemmatize using POS tagging\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = html.unescape(str(text))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "\n",
        "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "df[\"clean_description\"] = df[\"description\"].apply(clean_text)\n",
        "\n",
        "df[\"desc_word_count_before\"] = df[\"description\"].fillna(\"\").apply(lambda x: len(x.split()))\n",
        "df[\"desc_word_count_after\"] = df[\"clean_description\"].fillna(\"\").apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"\\nAverage word count before cleaning:\", df[\"desc_word_count_before\"].mean())\n",
        "print(\"Average word count after cleaning:\", df[\"desc_word_count_after\"].mean())\n",
        "\n",
        "sample = df[[\"description\", \"clean_description\"]].dropna().sample(3, random_state=42)\n",
        "print(\"\\nSample before vs after cleaning:\\n\", sample)\n"
      ],
      "metadata": {
        "id": "Aeo65y7NMfzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Day 5: Logistic Regression Model for Fake Job Detection\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# Load dataset (preprocessed with clean_description)\n",
        "#df = pd.read_csv('fake_job_postings.csv')\n",
        "df = df.dropna(subset=['clean_description'])\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['clean_description'])\n",
        "y = df['fraudulent']\n",
        "# Split data into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "#  Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# 6Ô∏è‚É£ Check example predictions\n",
        "test_samples = [\n",
        "    \"Work from home! Limited vacancies. Apply now.\",\n",
        "    \"We are hiring a data scientist for our Bangalore office.\"\n",
        "]\n",
        "sample_features = vectorizer.transform(test_samples)\n",
        "print(\"\\nSample Predictions:\", model.predict(sample_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr401gJoKi2h",
        "outputId": "567b889a-56cb-486b-ae90-e90a6f865f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9683827644096251\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      3401\n",
            "           1       0.98      0.35      0.52       173\n",
            "\n",
            "    accuracy                           0.97      3574\n",
            "   macro avg       0.98      0.68      0.75      3574\n",
            "weighted avg       0.97      0.97      0.96      3574\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[3400    1]\n",
            " [ 112   61]]\n",
            "\n",
            "Sample Predictions: [0 0]\n"
          ]
        }
      ]
    }
  ]
}